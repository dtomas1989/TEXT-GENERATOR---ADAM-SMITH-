{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libreries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/bin/.virtualenvs/david/lib/python3.6/site-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\n",
    "session = tf.compat.v1.InteractiveSession(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show a fragment of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The greatest improvements in the productive powers of labour, and the greater part of the skill, dexterity, and judgment, with which it is anywhere directed, or applied, seem to have been the effects of the division of labour. The effects of the division of labour, in the general business of society, will be more easily understood, by considering in what manner it operates in some particular manufactures. It is commonly supposed to be carried furthest in some very trifling ones; not perhaps that it really is carried further in them than in others of more importance: but in those trifling manufactures which are destined to supply the small wants of but a small number of people, the whole number of workmen must necessarily be small; and those employed in every different branch of the work can often be collected into the same workhouse, and placed at once under the view of the spectator.\r\n",
      "\r\n",
      "In those great manufactures, on the contrary, which are destined to supply the great wants of the great body of the people, every different branch of the work employs so great a number of workmen, that it is impossible to collect them all into the same workhouse. We can seldom see more, at one time, than those employed in one single branch. Though in such manufactures, therefore, the work may really be divided into a much greater number of parts, than in those of a more trifling nature, the division is not near so obvious, and has accordingly been much less observed.\r\n",
      "\r\n",
      "To take an example, therefore, from a very trifling manufacture, but one in which the division of labour has been very often taken notice of, the trade of a pin-maker: a workman not educated to this business (which the division of labour has rendered a distinct trade), nor acquainted with the use of the machinery employed in it (to the invention of which the same division of labour has probably given occasion), could scarce, perhaps, with his utmost industry, make one pin in a day, and certainly could not make twenty. But in the way in which this business is now carried on, not only the whole work is a peculiar trade, but it is divided into a number of branches, of which the greater part are likewise peculiar trades. One man draws out the wire; another straights it; a third cuts it; a fourth points it; a fifth grinds it at the top for receiving the head; to make the head requires two or three distinct operations; to put it on is a peculiar business; to whiten the pins is another; it is even a trade by itself to put them into the paper; and the important business of making a pin is, in this manner, divided into about eighteen distinct operations, which, in some manufactories, are all performed by distinct hands, though in others the same man will sometimes perform two or three of them. I have seen a small manufactory of this kind, where ten men only were employed, and where some of them consequently performed two or three distinct operations. But though they were very poor, and therefore but indifferently accommodated with the necessary machinery, they could, when they exerted themselves, make among them about twelve pounds of pins in a day. There are in a pound upwards of four thousand pins of a middling size. Those ten persons, therefore, could make among them upwards of forty-eight thousand pins in a day. Each person, therefore, making a tenth part of forty-eight thousand pins, might be considered as making four thousand eight hundred pins in a day. But if they had all wrought separately and independently, and without any of them having been educated to this peculiar business, they certainly could not each of them have made twenty, perhaps not one pin in a day; that is, certainly, not the two hundred and fortieth, perhaps not the four thousand eight hundredth, part of what they are at present capable of performing, in consequence of a proper division and combination of their different operations.\r\n",
      "\r\n",
      "In every other art and manufacture, the effects of the division of labour are similar to what they are in this very trifling one, though, in many of them, the labour can neither be so much subdivided, nor reduced to so great a simplicity of operation. The division of labour, however, so far as it can be introduced, occasions, in every art, a proportionable increase of the productive powers of labour. The separation of different trades and employments from one another, seems to have taken place in consequence of this advantage. This separation, too, is generally carried furthest in those countries which enjoy the highest degree of industry and improvement; what is the work of one man, in a rude state of society, being generally that of several in an improved one. In every improved society, the farmer is generally nothing but a farmer; the manufacturer, nothing but a manufacturer. The labour, too, which is necessary to produce any one complete manufacture, is almost always divided among a great number of hands. How many different trades are employed in each branch of the linen and woollen manufactures, from the growers of the flax and the wool, to the bleachers and smoothers of the linen, or to the dyers and dressers of the cloth! The nature of agriculture, indeed, does not admit of so many subdivisions of labour, nor of so complete a separation of one business from another, as manufactures. It is impossible to separate so entirely the business of the grazier from that of the corn-farmer, as the trade of the carpenter is commonly separated from that of the smith. The spinner is almost always a distinct person from the weaver; but the ploughman, the harrower, the sower of the seed, and the reaper of the corn, are often the same. The occasions for those different sorts of labour returning with the different seasons of the year, it is impossible that one man should be constantly employed in any one of them. This impossibility of making so complete and entire a separation of all the different branches of labour employed in agriculture, is perhaps the reason why the improvement of the productive powers of labour, in this art, does not always keep pace with their improvement in manufactures. The most opulent nations, indeed, generally excel all their neighbours in agriculture as well as in manufactures; but they are commonly more distinguished by their superiority in the latter than in the former. Their lands are in general better cultivated, and having more labour and expense bestowed upon them, produce more in proportion to the extent and natural fertility of the ground. But this superiority of produce is seldom much more than in proportion to the superiority of labour and expense. In agriculture, the labour of the rich country is not always much more productive than that of the poor; or, at least, it is never so much more productive, as it commonly is in manufactures. The corn of the rich country, therefore, will not always, in the same degree of goodness, come cheaper to market than that of the poor. The corn of Poland, in the same degree of goodness, is as cheap as that of France, notwithstanding the superior opulence and improvement of the latter country. The corn of France is, in the corn-provinces, fully as good, and in most years nearly about the same price with the corn of England, though, in opulence and improvement, France is perhaps inferior to England. The corn-lands of England, however, are better cultivated than those of France, and the corn-lands of France are said to be much better cultivated than those of Poland. But though the poor country, notwithstanding the inferiority of its cultivation, can, in some measure, rival the rich in the cheapness and goodness of its corn, it can pretend to no such competition in its manufactures, at least if those manufactures suit the soil, climate, and situation, of the rich country. The silks of France are better and cheaper than those of England, because the silk manufacture, at least under the present high duties upon the importation of raw silk, does not so well suit the climate of England as that of France. But the hardware and the coarse woollens of England are beyond all comparison superior to those of France, and much cheaper, too, in the same degree of goodness. In Poland there are said to be scarce any manufactures of any kind, a few of those coarser household manufactures excepted, without which no country can well subsist.\r\n",
      "\r\n",
      "This great increase in the quantity of work, which, in consequence of the division of labour, the same number of people are capable of performing, is owing to three different circumstances; first, to the increase of dexterity in every particular workman; secondly, to the saving of the time which is commonly lost in passing from one species of work to another; and, lastly, to the invention of a great number of machines which facilitate and abridge labour, and enable one man to do the work of many.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head Adam_Smith_An_Inquiry_into_the_Nature_and_Causes_of_the_Wealth_of_Nations.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('Adam_Smith_An_Inquiry_into_the_Nature_and_Causes_of_the_Wealth_of_Nations.txt').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data.lower().split(\"\\n\")\n",
    "corpus = [line for line in corpus if line != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = len(tokenizer.word_index) +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "\n",
    "for line in corpus:\n",
    "    token_sent = tokenizer.texts_to_sequences([line])[0]\n",
    "    #print(token_sent)\n",
    "    for i in range(1,len(token_sent)):\n",
    "        my_gram_seq = token_sent[:i+1]\n",
    "        #print(my_gram_seq)\n",
    "        input_data.append(my_gram_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pad Sequences & labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = max([len(x) for x in input_data])\n",
    "input_data = np.array(pad_sequences(input_data, maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   0   1 248]\n",
      " [  0   0   0 ...   1 248 143]\n",
      " [  0   0   0 ... 248 143   3]\n",
      " ...\n",
      " [  0   0   0 ...   2 203  74]\n",
      " [  0   0   0 ... 203  74 809]\n",
      " [  0   0   0 ...  74 809 810]]\n"
     ]
    }
   ],
   "source": [
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X , y = input_data[:,:-1], input_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat = tf.keras.utils.to_categorical(y, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Bidirectional(LSTM(150)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(total_words, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 763, 100)          81100     \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 300)               301200    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 811)               244111    \n",
      "=================================================================\n",
      "Total params: 626,411\n",
      "Trainable params: 626,411\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3484 samples\n",
      "Epoch 1/25\n",
      "3484/3484 [==============================] - 11s 3ms/sample - loss: 5.8934 - accuracy: 0.0795\n",
      "Epoch 2/25\n",
      "3484/3484 [==============================] - 9s 2ms/sample - loss: 5.5284 - accuracy: 0.0878\n",
      "Epoch 3/25\n",
      "3484/3484 [==============================] - 8s 2ms/sample - loss: 5.3741 - accuracy: 0.1042\n",
      "Epoch 4/25\n",
      "3484/3484 [==============================] - 9s 2ms/sample - loss: 5.0992 - accuracy: 0.1300\n",
      "Epoch 5/25\n",
      "3484/3484 [==============================] - 8s 2ms/sample - loss: 4.7562 - accuracy: 0.1564\n",
      "Epoch 6/25\n",
      "3484/3484 [==============================] - 8s 2ms/sample - loss: 4.4017 - accuracy: 0.1857\n",
      "Epoch 7/25\n",
      "3484/3484 [==============================] - 8s 2ms/sample - loss: 4.0564 - accuracy: 0.2118\n",
      "Epoch 8/25\n",
      "3484/3484 [==============================] - 8s 2ms/sample - loss: 3.7184 - accuracy: 0.2402\n",
      "Epoch 9/25\n",
      "3484/3484 [==============================] - 8s 2ms/sample - loss: 3.4129 - accuracy: 0.2807\n",
      "Epoch 10/25\n",
      "3484/3484 [==============================] - 9s 2ms/sample - loss: 3.1217 - accuracy: 0.3172\n",
      "Epoch 11/25\n",
      "3484/3484 [==============================] - 9s 2ms/sample - loss: 2.8482 - accuracy: 0.3596\n",
      "Epoch 12/25\n",
      "3484/3484 [==============================] - 8s 2ms/sample - loss: 2.5978 - accuracy: 0.4093\n",
      "Epoch 13/25\n",
      "3484/3484 [==============================] - 9s 2ms/sample - loss: 2.3664 - accuracy: 0.4644\n",
      "Epoch 14/25\n",
      "3484/3484 [==============================] - 9s 2ms/sample - loss: 2.1418 - accuracy: 0.5339\n",
      "Epoch 15/25\n",
      "3484/3484 [==============================] - 9s 2ms/sample - loss: 1.9266 - accuracy: 0.5873\n",
      "Epoch 16/25\n",
      "3484/3484 [==============================] - 9s 2ms/sample - loss: 1.7395 - accuracy: 0.6343\n",
      "Epoch 17/25\n",
      "3484/3484 [==============================] - 8s 2ms/sample - loss: 1.5621 - accuracy: 0.6837\n",
      "Epoch 18/25\n",
      "3484/3484 [==============================] - 8s 2ms/sample - loss: 1.4041 - accuracy: 0.7233\n",
      "Epoch 19/25\n",
      "3484/3484 [==============================] - 9s 2ms/sample - loss: 1.2526 - accuracy: 0.7778\n",
      "Epoch 20/25\n",
      "3484/3484 [==============================] - 9s 2ms/sample - loss: 1.1157 - accuracy: 0.8123\n",
      "Epoch 21/25\n",
      "3484/3484 [==============================] - 8s 2ms/sample - loss: 0.9979 - accuracy: 0.8404\n",
      "Epoch 22/25\n",
      "3484/3484 [==============================] - 9s 2ms/sample - loss: 0.8881 - accuracy: 0.8691\n",
      "Epoch 23/25\n",
      "3484/3484 [==============================] - 9s 2ms/sample - loss: 0.7889 - accuracy: 0.8927\n",
      "Epoch 24/25\n",
      "3484/3484 [==============================] - 8s 2ms/sample - loss: 0.6968 - accuracy: 0.9176\n",
      "Epoch 25/25\n",
      "3484/3484 [==============================] - 8s 2ms/sample - loss: 0.6147 - accuracy: 0.9348\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y_cat, epochs=25, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Train Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict next 5 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from the growers of the flax and the wool, to the bleachers and common smith which the quantity\n"
     ]
    }
   ],
   "source": [
    "my_text = \"from the growers of the flax and the wool, to the bleachers and\"\n",
    "next_words = 5\n",
    "  \n",
    "for _ in range(next_words):\n",
    "    token_sent = tokenizer.texts_to_sequences([my_text])[0]\n",
    "    token_sent = pad_sequences([token_sent], maxlen=max_sequence_len-1, padding='pre')\n",
    "    prediction = model.predict_classes(token_sent, verbose=0)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == prediction:\n",
    "            output_word = word\n",
    "            break\n",
    "    my_text += \" \" + output_word\n",
    "print(my_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict next 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from the growers of the flax and the wool, to the bleachers and common smith which the quantity of the people of the\n"
     ]
    }
   ],
   "source": [
    "my_text = \"from the growers of the flax and the wool, to the bleachers and\"\n",
    "next_words = 10\n",
    "  \n",
    "for _ in range(next_words):\n",
    "    token_sent = tokenizer.texts_to_sequences([my_text])[0]\n",
    "    token_sent = pad_sequences([token_sent], maxlen=max_sequence_len-1, padding='pre')\n",
    "    prediction = model.predict_classes(token_sent, verbose=0)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == prediction:\n",
    "            output_word = word\n",
    "            break\n",
    "    my_text += \" \" + output_word\n",
    "print(my_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict next 15 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from the growers of the flax and the wool, to the bleachers and common smith which the quantity of the people of the corn of labour is so\n"
     ]
    }
   ],
   "source": [
    "my_text = \"from the growers of the flax and the wool, to the bleachers and\"\n",
    "next_words = 15\n",
    "  \n",
    "for _ in range(next_words):\n",
    "    token_sent = tokenizer.texts_to_sequences([my_text])[0]\n",
    "    token_sent = pad_sequences([token_sent], maxlen=max_sequence_len-1, padding='pre')\n",
    "    prediction = model.predict_classes(token_sent, verbose=0)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == prediction:\n",
    "            output_word = word\n",
    "            break\n",
    "    my_text += \" \" + output_word\n",
    "print(my_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
